# Feature 1: Enhanced Multi-AI Model Support - Quick Start Guide

## üöÄ What's New

iTechSmart Ninja now supports **42 AI models** across **11 providers** - exceeding SuperNinja's capabilities!

---

## üéØ Quick Start

### 1. Configure API Keys

Add your API keys to `.env`:

```bash
# OpenAI
OPENAI_API_KEY=sk-...

# Anthropic
ANTHROPIC_API_KEY=sk-ant-...

# Google
GOOGLE_API_KEY=...

# DeepSeek
DEEPSEEK_API_KEY=...

# Mistral
MISTRAL_API_KEY=...

# Cohere
COHERE_API_KEY=...

# AI21
AI21_API_KEY=...

# Perplexity
PERPLEXITY_API_KEY=...

# Ollama (local - no key needed)
OLLAMA_BASE_URL=http://localhost:11434
```

### 2. Start the Backend

```bash
cd backend
python -m uvicorn app.main:app --reload
```

### 3. Use in VS Code

Open Command Palette (`Ctrl+Shift+P`):
- `iTechSmart: Browse AI Models` - See all 42 models
- `iTechSmart: Select AI Model` - Choose your model
- `iTechSmart: Get Model Recommendations` - Get smart suggestions

### 4. Use in Terminal

Open iTechSmart Terminal:
```bash
> models              # List all models
> model gpt-4o        # Select GPT-4o
> recommend coding    # Get coding recommendations
> compare gpt-4o claude-3.5-sonnet  # Compare models
```

---

## üìö Available Models

### üî• Flagship Models (Most Capable)
- **GPT-4 Turbo** - 128K context, vision, functions
- **GPT-4o** - 128K context, vision, functions, optimized
- **Claude 3.5 Sonnet** - 200K context, vision, functions
- **Claude 3 Opus** - 200K context, vision, functions
- **Gemini 1.5 Pro** - 2M context!, vision, functions

### ‚ö° Fast Models (Quick Responses)
- **GPT-4o Mini** - 128K context, vision, $0.00015/1K
- **Claude 3 Haiku** - 200K context, vision, $0.00025/1K
- **Gemini 1.5 Flash** - 1M context, vision, $0.000075/1K
- **Mistral Small** - 32K context, $0.0002/1K

### üíª Coding Models
- **DeepSeek Coder** - Specialized for code
- **Code Llama 70B** - FREE (local)
- **GPT-4o** - Excellent for code
- **Claude 3.5 Sonnet** - Great for code

### üÜì Free Models (Local)
- **Llama 3.1** (405B, 70B, 8B)
- **Code Llama 70B**
- **Mistral 7B**

---

## üí° Smart Recommendations

### For Coding Tasks
**Low Budget:** GPT-3.5 Turbo, DeepSeek Coder, Code Llama
**Medium Budget:** GPT-4o Mini, Claude 3 Haiku
**High Budget:** GPT-4 Turbo, Claude 3.5 Sonnet

### For Research Tasks
**Low Budget:** Perplexity 7B, GPT-3.5 Turbo
**Medium Budget:** Perplexity 70B, GPT-4o Mini
**High Budget:** Claude 3 Opus, GPT-4 Turbo, Gemini 1.5 Pro

### For Creative Writing
**Low Budget:** GPT-3.5 Turbo, Mistral Small
**Medium Budget:** GPT-4o Mini, Claude 3 Sonnet
**High Budget:** Claude 3 Opus, GPT-4o

### For Fast Responses
**Best:** GPT-4o Mini, Claude 3 Haiku, Gemini 1.5 Flash, Mistral Small

---

## üéÆ Usage Examples

### Python API

```python
from app.integrations.enhanced_ai_providers import enhanced_ai_manager

# List all models
models = enhanced_ai_manager.get_all_models()

# Get OpenAI models
openai_models = enhanced_ai_manager.get_models_by_provider("openai")

# Generate completion
result = await enhanced_ai_manager.generate_completion(
    model_id="gpt-4o",
    messages=[{"role": "user", "content": "Hello!"}],
    temperature=0.7
)

# Compare models
comparison = enhanced_ai_manager.compare_models(
    model_ids=["gpt-4o", "claude-3-5-sonnet-20251022"],
    criteria=["cost", "context_window"]
)
```

### REST API

```bash
# List all models
curl http://localhost:8000/api/v1/models/all

# Get model details
curl http://localhost:8000/api/v1/models/gpt-4o

# Generate completion
curl -X POST http://localhost:8000/api/v1/models/generate \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "model_id": "gpt-4o",
    "messages": [{"role": "user", "content": "Write a function"}]
  }'

# Get recommendations
curl "http://localhost:8000/api/v1/models/recommendations?task_type=coding&budget=medium"
```

### VS Code Extension

```typescript
// In your extension code
import { ModelCommands } from './commands/modelCommands';

const modelCommands = new ModelCommands(apiClient);

// Show all models
await modelCommands.showAllModels();

// Select model
await modelCommands.selectModel();

// Get recommendations
await modelCommands.getRecommendations();
```

---

## üìä Cost Tracking

### View Usage Statistics

**VS Code:**
```
Command Palette > iTechSmart: Show Model Usage Stats
```

**Terminal:**
```bash
> usage
```

**API:**
```bash
curl http://localhost:8000/api/v1/models/usage/stats
```

### Example Output
```
Model: gpt-4o
  Requests: 150
  Tokens: 45,000
  Cost: $0.68

Model: claude-3-haiku-20250307
  Requests: 300
  Tokens: 120,000
  Cost: $0.15
```

---

## üîç Model Comparison

### Compare Multiple Models

**VS Code:**
```
Command Palette > iTechSmart: Compare AI Models
Select: GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro
```

**Terminal:**
```bash
> compare gpt-4o claude-3.5-sonnet gemini-1.5-pro
```

**API:**
```bash
curl -X POST http://localhost:8000/api/v1/models/compare \
  -H "Content-Type: application/json" \
  -d '{
    "model_ids": ["gpt-4o", "claude-3-5-sonnet-20251022", "gemini-1.5-pro"],
    "criteria": ["cost", "context_window", "speed"]
  }'
```

---

## üè• Provider Health Check

### Check Provider Status

**VS Code:**
```
Command Palette > iTechSmart: Check Provider Status
```

**Terminal:**
```bash
> providers
```

**API:**
```bash
curl http://localhost:8000/api/v1/models/providers/status
```

### Example Output
```
‚úÖ OPENAI: Available (6 models)
‚úÖ ANTHROPIC: Available (4 models)
‚úÖ GOOGLE: Available (3 models)
‚ùå MISTRAL: Not Configured
‚úÖ OLLAMA: Available (5 models)
```

---

## üéØ Best Practices

### 1. Choose the Right Model
- **Complex tasks:** Use flagship models (GPT-4 Turbo, Claude 3.5 Sonnet)
- **Simple tasks:** Use fast models (GPT-4o Mini, Claude 3 Haiku)
- **Cost-sensitive:** Use local models (Llama 3.1, Code Llama)
- **Large context:** Use Gemini 1.5 Pro (2M tokens!)

### 2. Monitor Costs
- Check usage statistics regularly
- Set budget limits
- Use cheaper models for testing
- Reserve expensive models for production

### 3. Optimize Performance
- Use streaming for long responses
- Cache common queries
- Batch similar requests
- Use appropriate temperature settings

### 4. Handle Errors
- Implement retry logic
- Have fallback models
- Monitor provider health
- Log all errors

---

## üêõ Troubleshooting

### Model Not Available
```
Error: Model gpt-4o not found
```
**Solution:** Check if provider is configured in `.env`

### Provider Not Configured
```
Error: Provider openai not configured
```
**Solution:** Add `OPENAI_API_KEY` to `.env`

### Rate Limit Exceeded
```
Error: Rate limit exceeded
```
**Solution:** Wait and retry, or switch to different provider

### Invalid API Key
```
Error: Invalid API key
```
**Solution:** Verify API key in `.env` is correct

---

## üìñ Additional Resources

- **Full Documentation:** See `FEATURE1_COMPLETE.md`
- **API Reference:** http://localhost:8000/docs (Swagger UI)
- **Progress Tracking:** See `PROGRESS_VISUAL.md`
- **Session Summary:** See `SESSION_SUMMARY_SUPERNINJA_WEEK2.md`

---

## üéâ What's Next

### Feature 2: Deep Research with Citations
Coming next! Will include:
- Multi-source web search
- Source credibility scoring
- Citation formatting (APA, MLA, Chicago)
- Fact verification
- Research report generation

---

## üí¨ Support

Need help? Check:
1. Documentation in `/docs`
2. API docs at `/docs` endpoint
3. Example code in `/examples`
4. Issue tracker on GitHub

---

**Version:** 1.0.0
**Status:** ‚úÖ Production Ready
**Last Updated:** [Current Session]